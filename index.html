<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Predicción con el algoritmo XGBoost: Una implementación en R</title>

<script src="xgboost_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="xgboost_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="xgboost_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="xgboost_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="xgboost_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="xgboost_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="xgboost_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="xgboost_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="xgboost_files/navigation-1.1/tabsets.js"></script>
<link href="xgboost_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="xgboost_files/highlightjs-9.12.0/highlight.js"></script>
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@jonaslindeloev">
<meta name="twitter:title" content="Common statistical tests are linear models (or: how to teach stats)">
<meta name="twitter:image" content="https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1026978-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  
  gtag('config', 'UA-1026978-2');
</script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Predicción con el algoritmo XGBoost: Una implementación en R</h1>

</div>


<p><link rel="stylesheet" type="text/css" href="include/style.css"></p>
<!-- From https://stackoverflow.com/a/37839683/1297830 -->
<link rel="stylesheet" type="text/css" href="include/hideOutput.css">
<script src="include/hideOutput.js"></script>
<p>© José Rafael Caro Barrera (<a href="https://github.com/jrcarob">github</a>).<br /> Última actualización: 15 Junio, 2020 (Ver <a href="https://github.com/jrcarob">changelog</a>).<br /> Versión en Python (en desarrollo)</p>
<!-- Social sharing. From simplesharebuttons.com -->
<style type="text/css">
  #share-buttons img {
    width: 40px;
    padding-right: 15px;
    border: 0;
    box-shadow: 0;
    display: inline;
    vertical-align: top;
  }
</style>
<div id="share-buttons">
<!-- Twitter -->
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><a href="https://twitter.com" class="twitter-hashtag-button" data-size="large" data-related="jrcaro" data-show-count="false">Compartir en Twitter</a>    <!-- Email --><a href="mailto:?Subject=Predicción con XGBoost&amp/"><img src="https://simplesharebuttons.com/images/somacro/email.png" alt="Email" /></a></p>
</div>
<p><br /></p>
<div id="introducción-al-algoritmo-xgboost" class="section level1">
<h1><span class="header-section-number">1</span> Introducción al algoritmo XGBoost</h1>
<p>XGBoost, abreviatura de <em>Extreme Gradient Boosting</em>, es uno de los algoritmos de <em>machine learning</em> de tipo supervisado más usados en la actualidad y que utiliza el principio del <em>boosting</em>.</p>
<p>El aprendizaje supervisado es aquel que tiene variables de entrada <span class="math inline">\((x)\)</span> y una variable de salida <span class="math inline">\((Y)\)</span> y utiliza un algoritmo para aprender la función de mapeo de la entrada a la salida <span class="math display">\[Y=f(X)\]</span> El objetivo es aproximar la función de mapeo lo más precisa posible tal que cuando tenga nuevos datos de entrada <span class="math inline">\((x)\)</span> se puedan predecir las variables de salida <span class="math inline">\((Y)\)</span> para esos datos.</p>
<p>Se llama aprendizaje supervisado porque el proceso de un algoritmo que aprende del conjunto de datos de capacitación puede considerarse como un maestro que supervisa el proceso de aprendizaje. Conocemos las respuestas correctas, el algoritmo realiza predicciones de forma iterativa sobre los datos de entrenamiento y es corregido por el profesor. El aprendizaje se detiene cuando el algoritmo alcanza un nivel aceptable de rendimiento.</p>
<p>Los problemas de aprendizaje supervisados pueden agruparse en problemas de regresión y clasificación.</p>
<p><span class="math inline">\(\bullet\)</span> <strong>Clasificación:</strong> Un problema de clasificación es cuando la variable de salida es una categoría, como “rojo” o “azul” o “enfermedad” y “sin enfermedad”.</p>
<p><span class="math inline">\(\bullet\)</span> <strong>Regresión:</strong> Un problema de regresión es cuando la variable de salida es un valor real, como “euros” o “kilogramos”. Algunos tipos comunes de problemas construidos sobre la clasificación y la regresión incluyen la recomendación y la predicción de series de tiempo, respectivamente. Algunos ejemplos populares de algoritmos supervisados de aprendizaje automático son:</p>
<p><strong><em>Regresión lineal</em></strong> para problemas de regresión.</p>
<p><strong><em>Random forest</em></strong> para problemas de clasificación y regresión.</p>
<p><strong><em>Support vector machine</em></strong> para problemas de clasificación.</p>
<p>El algoritmo <em>xgboost</em> es similar al <em>gradient boosting</em> pero más eficiente. Tiene soluciones de modelos lineales y algoritmos de aprendizaje en árbol y es al menos 10 veces más rápido que las implementaciones existentes del <em>gradient boosting</em>. Es compatible con varias funciones objetivas, incluida la regresión, clasificación y clasificación. y lo que lo hace rápido es su capacidad para hacer cálculos paralelos en una sola máquina.</p>
<p>En términos de eficiencia, precisión y factibilidad es más potente que el algoritmo <em>random forest</em>, por ejemplo o una red neuronal y dado que tiene un poder predictivo muy alto pero una implementación relativamente lenta, el <em>xgboost</em> se convierte en una opción idónea para resolver la mayoría de los problemas de regresión, clasificación y clasificación, así como las funciones objetivas creadas por el usuario. También tiene características adicionales para hacer validación cruzada y encontrar aquéllas variables más importantes por lo que lo hace interesante para modelos de <strong>credit scoring</strong>, <strong>reclamaciones de seguros</strong>, o donde hay muchos parámetros que deben controlarse para optimizar el modelo.</p>
<p>Como se puede observar en el siguiente gráfico (1), el modelo XGBoost tiene la mejor combinación de rendimiento de predicción y tiempo de procesamiento en comparación con otros algoritmos.</p>
<hr />
<p><a href="performance.png"><img src="performance.png" /></a></p>
<hr />
<p>Chollet y Allaire, (2018) (2) resumen el valor de XGBoost como sigue:</p>
<blockquote>
<p>“<em>XGBoost</em> se usa para problemas donde hay la disponibilidad de datos estructurados es muy amplia, mientras que el <em>deep learning</em> se usa para problemas de percepción como la clasificación de imágenes. Los usuarios de la primera casi siempre usan la biblioteca XGBoost.”</p>
</blockquote>
<blockquote>
<p>“Estas son las dos técnicas con las que se debería estar más familiarizado para tener éxito en el aprendizaje automático aplicado hoy: máquinas de aumento de gradiente, para problemas de aprendizaje superficial; y aprendizaje profundo, para problemas de percepción. En términos técnicos, esto significa que se deberá estar familiarizado con XGBoost y Keras.”</p>
</blockquote>
<blockquote>
<footer>
– Francoise Chollet y J.J. Allaire
</footer>
</blockquote>
<p>Originalmente, XGBoost es una librería escrita en C++ y exportada a <strong>R</strong> en el paquete <code>xgboost</code> en nuestro caso, se ha entrenado el modelo XGBoost usando el paquete en <strong>R</strong> <code>caret</code>(3).</p>
</div>
<div id="objetivos-y-preparación-de-los-datos" class="section level1">
<h1><span class="header-section-number">2</span> Objetivos y preparación de los datos</h1>
<p>En este post y avance preliminar de proyecto con aplicación a datos al mercado inmobiliario español se entrena y pone a punto un modelo basado en el algoritmo XGBoost usando la librería <em>tidymodels</em> de <code>R</code>. Para ello y como fuente de datos usamos el dataset <a href="https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf">AmesHousing</a> (4) que contiene datos de 82 variables para 2.930 propiedades en el condado de Ames, Iowa. Nuestro modelo intentará predecir el precio de venta de la vivienda.</p>
<div class="fold s">
<pre class="r"><code># carga de los datos
library(AmesHousing)

# librerías para la limpieza  y preparación de los datos
library(janitor)
library(dplyr)

# carga de los paquetes necesarios
library(rsample)
library(recipes)
library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)

# aceleración de los cálculos con procesamiento paralelo (opcional pero útil)
library(doParallel)
all_cores &lt;- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)</code></pre>
</div>
<div id="carga-de-los-datos-a-modelizar" class="section level3">
<h3><span class="header-section-number">2.0.1</span> Carga de los datos a modelizar</h3>
<div class="fold s">
<pre class="r"><code># fijamos la semilla aleatoria set.seed() para que los resultados 
# sean replicables y así podemos reproducir cualquier simulación.
set.seed(1234)

# carga de los datos y limpieza de los nombres
ames_data &lt;- make_ames() %&gt;%
  janitor::clean_names()</code></pre>
</div>
</div>
</div>
<div id="proceso-xgboost" class="section level1">
<h1><span class="header-section-number">3</span> Proceso XGBoost</h1>
<div id="paso-0-análisis-exploratorio-de-datos." class="section level2">
<h2><span class="header-section-number">3.1</span> Paso 0: Análisis Exploratorio de Datos.</h2>
<p>En este punto inicial, haríamos resúmenes de los datos y algunos gráficos simples para obtener una comprensión lo más detallada posible de los datos. Para simplificar, vamos a omitir este paso pero, en un análisis del mundo real, comprender los problemas comerciales y hacer un EDA efectivo son a menudo los aspectos cruciales que requieren más tiempo y análisis.</p>
</div>
<div id="paso-1-división-de-los-datos." class="section level2">
<h2><span class="header-section-number">3.2</span> Paso 1: División de los Datos.</h2>
<p>Ahora dividimos los datos en datos de entrenamiento y prueba. Los datos de entrenamiento se utilizan para el entrenamiento del modelo y el ajuste de hiperparámetros. Una vez entrenado, el modelo se puede evaluar contra los datos de prueba para evaluar la precisión. Normalmente se suelen utilizar un 80% de los datos para el entrenamiento del modelo, mientras que la simulación o test se hace con el 20% restante.</p>
<div class="fold s">
<pre class="r"><code># División de los datos para entrenamiento y prueba. Estratificación por el precio de venta.
ames_split &lt;- rsample::initial_split(
  ames_data, 
  prop = 0.8, 
  strata = sale_price
)</code></pre>
</div>
</div>
<div id="paso-2-pre-procesamiento." class="section level2">
<h2><span class="header-section-number">3.3</span> Paso 2: Pre-procesamiento.</h2>
<p>El preprocesamiento altera los datos para hacer que nuestro modelo sea más predictivo y el proceso de capacitación requiera menos cálculos computacionales. Muchos modelos requieren un preprocesamiento variable cuidadoso y extenso para producir predicciones precisas. Sin embargo, XGBoost es más robusto frente a datos muy asimétricos y/o correlacionados, por lo que la cantidad de preprocesamiento requerido con XGBoost es mínima. Sin embargo, todavía podemos hacer uso de algunos preprocesamientos, además, en R, con las librerías que <code>tidymodels</code> usa, utilizamos el paquete <code>recipes</code> para definir estos pasos de preprocesamiento anteriormente mencionados:</p>
<div class="fold s">
<pre class="r"><code># &quot;receta&quot; de preprocesamiento
preprocessing_recipe &lt;- 
  recipes::recipe(sale_price ~ ., data = training(ames_split)) %&gt;%
  # conversión de variables categóricas a factores 
  recipes::step_string2factor(all_nominal()) %&gt;%
  # combinación de niveles de factores de baja frecuencia
  recipes::step_other(all_nominal(), threshold = 0.01) %&gt;%
  # eliminación de predictores sin varianza que no suministran información predicitiva. 
  recipes::step_nzv(all_nominal()) %&gt;%
  prep()</code></pre>
</div>
<p>Como se puede ver en el gráfico a continuación, para la variable <code>neighborhood</code> (“vecindario”), varios de los niveles de factores con la menor cantidad de observaciones (menos del 1% del número total de observaciones) se han agrupado en un nivel de factor <code>other</code>. Hicimos este preprocesamiento con el comando <code>step_other()</code> en el apartado anterior.</p>
<p><a href="others.png"><img src="others.png" /></a></p>
</div>
<div id="paso-3-división-para-validación-cruzada." class="section level2">
<h2><span class="header-section-number">3.4</span> Paso 3: División para Validación Cruzada.</h2>
<p>Aplicamos el preprocesamiento previamente definida con el comando <code>bake()</code>. Luego, utilizamos la validación cruzada para dividir aleatoriamente los datos de entrenamiento en conjuntos de entrenamiento y pruebas adicionales. Utilizaremos estos pliegues de validación cruzada adicionales para ajustar nuestros hiperparámetros en un paso posterior.</p>
<div class="fold s">
<pre class="r"><code>ames_cv_folds &lt;- 
  recipes::bake(
    preprocessing_recipe, 
    new_data = training(ames_split)
  ) %&gt;%  
  rsample::vfold_cv(v = 5)</code></pre>
</div>
</div>
<div id="paso-4-especificación-del-modelo-xgboost." class="section level2">
<h2><span class="header-section-number">3.5</span> Paso 4: Especificación del Modelo XGBoost.</h2>
<p>Usamos el paquete <code>parsnip</code> para definir la especificación del modelo XGBoost. A continuación, utilizamos <code>boost_tree()</code> junto con <code>tune()</code> para definir los hiperparámetros para someterlos a un ajuste en un paso posterior.</p>
<div class="fold s">
<pre class="r"><code># Especificación del modelo XGBoost
xgboost_model &lt;- 
  parsnip::boost_tree(
    mode = &quot;regression&quot;,
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %&gt;%
    set_engine(&quot;xgboost&quot;, objective = &quot;reg:squarederror&quot;)</code></pre>
</div>
</div>
<div id="paso-5-especificación-del-grid." class="section level2">
<h2><span class="header-section-number">3.6</span> Paso 5: Especificación del <em>grid</em>.</h2>
<p>A continuación, usamos el paquete <code>dials</code> para especificar el conjunto de paramétros.</p>
<div class="fold s">
<pre class="r"><code># Especificación del grid
xgboost_params &lt;- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )</code></pre>
</div>
<p>Luego configuramos el espacio del <em>grid</em>. Las funciones <code>dials::grid_*</code> admiten varios métodos para definir este espacio.El uso de la función <code>dials::grid_max_entropy()</code> cubre el espacio del hiperparámetro de manera que cualquier parte del espacio tenga una combinación observada que no se sitúe muy lejos de él.</p>
<div class="fold s">
<pre class="r"><code>xgboost_grid &lt;- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )

knitr::kable(head(xgboost_grid))</code></pre>
</div>
<table>
<thead>
<tr class="header">
<th align="center">min_n</th>
<th align="center">tree_depth</th>
<th align="center">learn_rate</th>
<th align="center">loss_reduction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">34</td>
<td align="center">1</td>
<td align="center">0.0118682</td>
<td align="center">29.9649253</td>
</tr>
<tr class="even">
<td align="center">38</td>
<td align="center">12</td>
<td align="center">0.0001291</td>
<td align="center">0.6156496</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">7</td>
<td align="center">0.0000949</td>
<td align="center">0.0000000</td>
</tr>
<tr class="even">
<td align="center">32</td>
<td align="center">4</td>
<td align="center">0.0000005</td>
<td align="center">0.0000367</td>
</tr>
<tr class="odd">
<td align="center">14</td>
<td align="center">2</td>
<td align="center">0.0001833</td>
<td align="center">0.0000000</td>
</tr>
<tr class="even">
<td align="center">31</td>
<td align="center">8</td>
<td align="center">0.0000000</td>
<td align="center">1.4345098</td>
</tr>
</tbody>
</table>
<p>Para ajustar nuestro modelo, realizamos una búsqueda del <em>grid</em> sobre el espacio de esa cuadrícula de <code>xgboost_grid</code> para identificar los valores de hiperparámetro que tienen el error de predicción más bajo.</p>
</div>
<div id="paso-6-definición-del-flujo-de-trabajo." class="section level2">
<h2><span class="header-section-number">3.7</span> Paso 6: Definición del Flujo de Trabajo.</h2>
<p>Utilizamos el nuevo paquete de flujos de trabajo <code>tidymodel</code> para agregar una fórmula a nuestra especificación del modelo XGBoost.</p>
<div class="fold s">
<pre class="r"><code>xgboost_wf &lt;- 
  workflows::workflow() %&gt;%
  add_model(xgboost_model) %&gt;% 
  add_formula(sale_price ~ .)</code></pre>
</div>
</div>
<div id="paso-7-ajuste-del-modelo." class="section level2">
<h2><span class="header-section-number">3.8</span> Paso 7: Ajuste del Modelo.</h2>
<p>El “ajuste” es donde el <em>ecosistema</em> de paquetes <code>tidymodels</code> realmente entra en funcionamiento. Aquí hay un desglose rápido de los objetos pasados a los primeros 4 argumentos de nuestra llamada a <code>tune_grid()</code> a continuación:</p>
<p><span class="math inline">\(\bullet\)</span> “object”: <code>xgboost_wf</code>, que es un flujo de trabajo que definimos por los paquetes <code>parsnip</code> y <code>workflows</code>.</p>
<p><span class="math inline">\(\bullet\)</span> “resamples”: <code>ames_cv_folds</code> según lo definido por los paquetes <code>rsample</code> y <code>recipes</code>.</p>
<p><span class="math inline">\(\bullet\)</span> “grid”: <code>xgboost_grid</code> nuestro espacio como lo define el paquete <code>dials</code>.</p>
<p><span class="math inline">\(\bullet\)</span> “metrics”: el paquete <code>yardstick</code> define el conjunto de métricas utilizado para evaluar el rendimiento del modelo.</p>
<div class="fold s">
<pre class="r"><code># ajuste de los hiperparámetros
xgboost_tuned &lt;- tune::tune_grid(
  object = xgboost_wf,
  resamples = ames_cv_folds,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)</code></pre>
</div>
<p>En el bloque de código anterior, <code>tune_grid()</code> realizó una búsqueda del <em>grid</em> en todas las 60 combinaciones de parámetros definidas con <code>xgboost_grid</code> y usó validación cruzada 5 veces junto con el <em>rmse</em> (error cuadrático medio), <em>rsq</em> (<span class="math inline">\(R^{2}\)</span>) y <em>mae</em> (error absoluto medio) para medir la precisión de predicción. Por lo tanto, nuestro ajuste solo se ajusta (valga la redundancia) a <span class="math inline">\(60\times 5=300\)</span> modelos XGBoost, cada uno con 1,000 árboles, todos en busca de los hiperparámetros óptimos. El cálculo fue considerablemente largo y no exento de algún problema que otro. A continuación se muestran los valores de hiperparámetro que se desempeñaron mejor para minimizar el error cuadrático medio:</p>
<div class="fold s">
<pre class="r"><code>xgboost_tuned %&gt;%
  tune::show_best(metric = &quot;rmse&quot;) %&gt;%
  knitr::kable()</code></pre>
</div>
<table>
<thead>
<tr class="header">
<th align="center">min_n</th>
<th align="center">tree_depth</th>
<th align="center">learn_rate</th>
<th align="center">loss_reduction</th>
<th align="center">.metric</th>
<th align="center">.estimator</th>
<th align="center">mean</th>
<th align="center">n</th>
<th align="center">std_err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">12</td>
<td align="center">7</td>
<td align="center">0.0346875</td>
<td align="center">0.0451186</td>
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">25561.99</td>
<td align="center">5</td>
<td align="center">2983.927</td>
</tr>
<tr class="even">
<td align="center">9</td>
<td align="center">13</td>
<td align="center">0.0183617</td>
<td align="center">0.1042750</td>
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">25576.99</td>
<td align="center">5</td>
<td align="center">2687.849</td>
</tr>
<tr class="odd">
<td align="center">23</td>
<td align="center">5</td>
<td align="center">0.0788798</td>
<td align="center">0.7513677</td>
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">25645.38</td>
<td align="center">5</td>
<td align="center">2461.057</td>
</tr>
<tr class="even">
<td align="center">11</td>
<td align="center">6</td>
<td align="center">0.0091690</td>
<td align="center">0.0000001</td>
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">25669.84</td>
<td align="center">5</td>
<td align="center">2706.457</td>
</tr>
<tr class="odd">
<td align="center">10</td>
<td align="center">2</td>
<td align="center">0.0108475</td>
<td align="center">0.0000003</td>
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">25883.23</td>
<td align="center">5</td>
<td align="center">2828.172</td>
</tr>
</tbody>
</table>
<p>A continuación, aislamos los valores con hiperparámetros de mejor rendimiento.</p>
<div class="fold s">
<pre class="r"><code>xgboost_best_params &lt;- xgboost_tuned %&gt;%
  tune::select_best(&quot;rmse&quot;)

knitr::kable(xgboost_best_params)</code></pre>
</div>
<table>
<thead>
<tr class="header">
<th align="center">min_n</th>
<th align="center">tree_depth</th>
<th align="center">learn_rate</th>
<th align="center">loss_reduction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">12</td>
<td align="center">7</td>
<td align="center">0.0346875</td>
<td align="center">0.0451186</td>
</tr>
</tbody>
</table>
<p>Finalizamos el modelo XGBoost usando el mejor parámetro que hemos ajustado.</p>
<div class="fold s">
<pre class="r"><code>xgboost_model_final &lt;- xgboost_model %&gt;% 
  finalize_model(xgboost_best_params)</code></pre>
</div>
</div>
<div id="paso-8-evaluación-del-rendimiento-con-los-datos-de-prueba." class="section level2">
<h2><span class="header-section-number">3.9</span> Paso 8: Evaluación del Rendimiento con los Datos de Prueba.</h2>
<p>Ahora que hemos entrenado nuestro modelo, necesitamos evaluar el rendimiento del mismo Utilizamos los datos de prueba del paso 1 (ese 20% de datos que no se usaron en el entrenamiento modelo) para evaluar el rendimiento.</p>
<p>Usamos las métricas <em>rmse</em> (Root Mean Squared Error), rsq (R Squared) y mae (Mean Absolute Value) del paquete <code>yardstick</code> en nuestra evaluación del modelo.</p>
<p>Primero, evaluamos las métricas de los datos de capacitación:</p>
<div class="fold s">
<pre class="r"><code>train_processed &lt;- bake(preprocessing_recipe,  new_data = training(ames_split))

train_prediction &lt;- xgboost_model_final %&gt;%
  # ajuste del modelo en todos los datos de entrenamiento
  fit(
    formula = sale_price ~ ., 
    data    = train_processed
  ) %&gt;%
  # predicción de los precios de venta en los datos de entrenamiento
  predict(new_data = train_processed) %&gt;%
  bind_cols(training(ames_split))

xgboost_score_train &lt;- 
  train_prediction %&gt;%
  yardstick::metrics(sale_price, .pred) %&gt;%
  mutate(.estimate = format(round(.estimate, 2), big.mark = &quot;,&quot;))

knitr::kable(xgboost_score_train)</code></pre>
</div>
<table>
<thead>
<tr class="header">
<th align="center">.metric</th>
<th align="center">estimator.</th>
<th align="center">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">3,807.24</td>
</tr>
<tr class="even">
<td align="center">rsq</td>
<td align="center">standard</td>
<td align="center">1.00</td>
</tr>
<tr class="odd">
<td align="center">mae</td>
<td align="center">standard</td>
<td align="center">2,747.17</td>
</tr>
</tbody>
</table>
<p>Y ahora para el resto de los datos:</p>
<div class="fold s">
<pre class="r"><code>test_processed  &lt;- bake(preprocessing_recipe, new_data = testing(ames_split))

test_prediction &lt;- xgboost_model_final %&gt;%
  # ajuste del modelo en todos los datos de entrenamiento
  fit(
    formula = sale_price ~ ., 
    data    = train_processed
  ) %&gt;%
  # uso del modelo de ajuste de entrenamiento para la predicción de los datos de prueba
  predict(new_data = test_processed) %&gt;%
  bind_cols(testing(ames_split))

# mmedición de la precisión de nuestro modelo usando `yardstick`
xgboost_score &lt;- 
  test_prediction %&gt;%
  yardstick::metrics(sale_price, .pred) %&gt;%
  mutate(.estimate = format(round(.estimate, 2), big.mark = &quot;,&quot;))

knitr::kable(xgboost_score)</code></pre>
</div>
<table>
<thead>
<tr class="header">
<th align="center">.metric</th>
<th align="center">estimator.</th>
<th align="center">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">rmse</td>
<td align="center">standard</td>
<td align="center">30,217.58</td>
</tr>
<tr class="even">
<td align="center">rsq</td>
<td align="center">standard</td>
<td align="center">0.87</td>
</tr>
<tr class="odd">
<td align="center">mae</td>
<td align="center">standard</td>
<td align="center">15,728.22</td>
</tr>
</tbody>
</table>
<p>Las métricas anteriores en los datos de la prueba son significativamente peores que las métricas de nuestros datos de entrenamiento, por lo que sabemos que hay algún ajuste excesivo en nuestro modelo. Esto resalta la importancia de usar datos de prueba, en lugar de datos de entrenamiento, para evaluar el desempeño del modelo.</p>
<p>Para comprobar rápidamente que no hay un problema con las predicciones de nuestro modelo, podemos obtener la gráfica de los residuos de datos de prueba:</p>
<div class="fold s">
<pre class="r"><code>house_prediction_residual &lt;- test_prediction %&gt;%
  arrange(.pred) %&gt;%
  mutate(residual_pct = (sale_price - .pred) / .pred) %&gt;%
  select(.pred, residual_pct)

ggplot(house_prediction_residual, aes(x = .pred, y = residual_pct)) +
  geom_point() +
  xlab(&quot;Predicted Sale Price&quot;) +
  ylab(&quot;Residual (%)&quot;) +
  scale_x_continuous(labels = scales::dollar_format()) +
  scale_y_continuous(labels = scales::percent)</code></pre>
</div>
<p><a href="residuals.png"><img src="residuals.png" /></a></p>
<p>La gráfica anterior no muestra tendencias muy obvias en los residuos. Esto indica que, a un nivel muy alto, nuestro modelo no está haciendo sistemáticamente predicciones inexactas para casas con ciertos precios de venta pronosticados. Haríamos más validación del modelo aquí para un análisis del mundo real, pero, por el bien de esta publicación, el gráfico anterior es lo suficientemente bueno para nuestro propósito.</p>
</div>
</div>
<div id="conclusiones" class="section level1">
<h1><span class="header-section-number">4</span> Conclusiones</h1>
<p>El objetivo de este análisis era trabajar a través del proceso de capacitación de un modelo XGBoost en <strong>R</strong> usando el paquete <code>tidymodels</code>, y aprender los conceptos básicos de funcionamiento del algoritmo, si bien no hemos hecho demasiado énfasis en el rendimiento de nuestro modelo, se han sentado las bases para futuras líneas de investigación con esta herramienta.</p>
<p>Hemos visto que la librería <code>tidymodels</code> nos brinda un proceso y herramientas estándar para manejar el remuestreo (<code>rsample</code>), el preprocesamiento de datos (<code>recipes</code>), la especificación del modelo (<code>parsnip</code>), el ajuste (<code>tune</code>) y la validación del modelo (<code>yardstick</code>). En este sentido, la capacidad <code>tidymodels</code> para “ordenar” el proceso de aprendizaje automático es una mejora de cambio gradual para la accesibilidad al aprendizaje automático en <strong>R</strong>; así, es más fácil entrenar y comprender el proceso de entrenamiento del modelo XGBoost.</p>
</div>
<div id="bibliografía-y-referencias" class="section level1">
<h1><span class="header-section-number">5</span> Bibliografía y referencias</h1>
<ol style="list-style-type: decimal">
<li><p><a href="https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d" class="uri">https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d</a></p></li>
<li><p>Chollet, F. y Allaire, J. J. (2018): <em>“Deep Learning with R,”</em> Ed. Manning Publications.</p></li>
<li><p>Kuhn, M. (2008). <em>“Building Predictive Models in R Using the caret Package,”</em> Journal of Statistical Software, 28(5), 1-26; <a href="doi:http://dx.doi.org/10.18637/jss.v028.i05" class="uri">doi:http://dx.doi.org/10.18637/jss.v028.i05</a>.</p></li>
<li><p>De Cock, D. (2011). <em>“Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project,”</em>: Journal of Statistics Education, Volume 19, Number 3.</p></li>
<li><p>Chen, T. y Guestrin, C. (2016): <em>“XGBoost: A Scalable Tree Boosting System,”</em> <a href="doi:10.1145/2939672.2939785" class="uri">doi:10.1145/2939672.2939785</a></p></li>
<li><p>Friedman, J.H. (2001): <em>“Greedy Function Approximation: A Gradient Boosting Machine,”</em> Annals of Statistics, pp. 1189–1232.</p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
